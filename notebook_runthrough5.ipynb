{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "We've designed a nice conversational retrieval chain over our docs - now let's put it into production! We'll go over how our newly constructed chain will interface with core native JavaScript web primitives used in popular frameworks like Express and Next.js: the global `Response` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"npm:dotenv/config\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back through our document preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Peer dependency\n",
    "import * as parse from \"npm:pdf-parse\";\n",
    "import { PDFLoader } from \"npm:langchain@0.0.202/document_loaders/fs/pdf\";\n",
    "import { RecursiveCharacterTextSplitter } from \"npm:langchain@0.0.202/text_splitter\";\n",
    "import { MemoryVectorStore } from \"npm:langchain@0.0.202/vectorstores/memory\";\n",
    "import { OpenAIEmbeddings } from \"npm:langchain@0.0.202/embeddings/openai\";\n",
    "\n",
    "const loader = new PDFLoader(\"./static/docs/MachineLearning-Lecture01.pdf\");\n",
    "\n",
    "const rawCS229Docs = await loader.load();\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 1536,\n",
    "  chunkOverlap: 128,\n",
    "});\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(rawCS229Docs);\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings();\n",
    "\n",
    "const vectorstore = new MemoryVectorStore(embeddings);\n",
    "\n",
    "await vectorstore.addDocuments(splitDocs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate the chain, starting with our familiar document retrieval component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableSequence } from \"npm:langchain@0.0.202/runnables\";\n",
    "\n",
    "const convertDocsToString = (documents: Document[]): string => {\n",
    "  return documents.map((document) => `<doc>\\n${document.pageContent}\\n</doc>`).join(\"\\n\");\n",
    "};\n",
    "\n",
    "const documentRetrievalChain = RunnableSequence.from([\n",
    "  (input) => input.standalone_question,\n",
    "  retriever,\n",
    "  convertDocsToString,\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the rephrase question step to handle followup questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"npm:langchain@0.0.202/chat_models/openai\";\n",
    "import { StringOutputParser } from \"npm:langchain@0.0.202/schema/output_parser\";\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"npm:langchain@0.0.202/prompts\";\n",
    "\n",
    "const REPHRASE_QUESTION_SYSTEM_TEMPLATE = `Using the provided chat history as context, rephrase the following question to be a standalone question that has no external references.\n",
    "\n",
    "Do not respond with anything other than a rephrased standalone question.`;\n",
    "\n",
    "const rephraseQuestionChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", REPHRASE_QUESTION_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\"human\", \"Now, answer the following question:\\n{question}\"],\n",
    "]);\n",
    "\n",
    "const rephraseQuestionChain = RunnableSequence.from([\n",
    "  rephraseQuestionChainPrompt,\n",
    "  new ChatOpenAI({ temperature: 0, modelName: \"gpt-3.5-turbo-1106\" }),\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, the final answer generation chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are an experienced researcher, expert at interpreting and answering questions based on provided sources.\n",
    "Using the below provided context and chat history, answer the user's question to the best of your ability using only the resources provided. Be concise!\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>`;\n",
    "\n",
    "const answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", ANSWER_CHAIN_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\"human\", \"Now, answer this question using the previous context and chat history:\\n{standalone_question}\"]\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before the final construction of our chain, we need to a make a slight change - the global JavaScript Web response object allows you to pass a stream, but it must be a binary stream instead of a stream of string chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// const responseObject = new Response(stream, {\n",
    "//   headers: {}\n",
    "// });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a different output parser to format our chain's response the way that this native object expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"The requirements for this course include familiarity with basic probability and statistics, as well \"\u001b[39m... 365 more characters,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m }\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"The requirements for this course include familiarity with basic probability and statistics, as well \"\u001b[39m... 365 more characters,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnablePassthrough } from \"npm:langchain@0.0.202/runnables\";\n",
    "\n",
    "const conversationalRetrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    standalone_question: rephraseQuestionChain,\n",
    "  }),\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationChainPrompt,\n",
    "  new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }),\n",
    "]);\n",
    "\n",
    "await conversationalRetrievalChain.invoke({\n",
    "  question: \"What are the prerequisites for this course?\",\n",
    "  history: []\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HttpResponseOutputParser } from \"npm:langchain@0.0.202/output_parsers\";\n",
    "import { RunnableWithMessageHistory } from \"npm:langchain@0.0.202/runnables\"; \n",
    "import { ChatMessageHistory } from \"npm:langchain@0.0.202/memory\";\n",
    "\n",
    "const httpResponseOutputParser = new HttpResponseOutputParser({\n",
    "  contentType: \"text/plain\"\n",
    "});\n",
    "\n",
    "const messageHistory = new ChatMessageHistory();\n",
    "\n",
    "const conversationalRetrievalChainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: conversationalRetrievalChain,\n",
    "  getMessageHistory: (_sessionId) => messageHistory,\n",
    "  inputMessageKey: \"question\",\n",
    "  historyMessagesKey: \"history\"\n",
    "}).pipe(httpResponseOutputParser);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on http://localhost:8080/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  finished: Promise { \u001b[36m<pending>\u001b[39m },\n",
       "  shutdown: \u001b[36m[AsyncFunction: shutdown]\u001b[39m,\n",
       "  ref: \u001b[36m[Function: ref]\u001b[39m,\n",
       "  unref: \u001b[36m[Function: unref]\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const port = 8080;\n",
    "\n",
    "const handler = async (request: Request): Response => {\n",
    "  const body = await request.json();\n",
    "  const question = body.question;\n",
    "  const sessionId = body.session_id;\n",
    "  const stream = await conversationalRetrievalChainWithHistory.stream({\n",
    "    question\n",
    "  }, { configurable: { sessionId } });\n",
    "  \n",
    "  return new Response(stream, {\n",
    "    status: 200,\n",
    "    headers: {\n",
    "      \"Content-Type\": \"text/plain\"\n",
    "    }\n",
    "  });\n",
    "};\n",
    "\n",
    "Deno.serve({ port }, handler);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our server is live, let's try calling it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "const decoder = new TextDecoder();\n",
    "\n",
    "// readChunks() reads from the provided reader and yields the results into an async iterable\n",
    "function readChunks(reader) {\n",
    "  return {\n",
    "    async* [Symbol.asyncIterator]() {\n",
    "      let readResult = await reader.read();\n",
    "      while (!readResult.done) {\n",
    "        yield decoder.decode(readResult.value);\n",
    "        readResult = await reader.read();\n",
    "      }\n",
    "    },\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "const response = await fetch(\"http://localhost:8080\", {\n",
    "  method: \"POST\",\n",
    "  body: JSON.stringify({\n",
    "    question: \"What are the prerequisites for this course?\",\n",
    "    session_id: \"1\",\n",
    "  }),\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\"\n",
    "  }\n",
    "});\n",
    "\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
