{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain.js\n",
    "\n",
    "Welcome! This short course will introduce you to [LangChain.js](https://github.com/langchain-ai/langchainjs), a framework for building large language model (LLM) powered, context-aware, reasoning applications. By the end of the course, you'll learn all the concepts you need to create your own version of a popular example of this type of app: a \"chat with data\" chain that lets you ask questions about a document's contents using natural language.\n",
    "\n",
    "This course follows on from the previous DeepLearning.ai courses on the Python version of LangChain. Much like LangChain.js itself, it's intended for web developers and others in the broader JavaScript ecosystem interested in building with LLMs, and will dive more deeply into features like streaming and integration with standard web/JavaScript APIs.\n",
    "\n",
    "This notebook uses the [Deno](https://deno.com/) Jupyter kernel.\n",
    "\n",
    "**Note:** Throughout this course, we'll link to some explorable traces in [LangSmith](https://smith.langchain.com/) that illustrate how the different example chains work. If you're following along locally, you'll need to set a few environment variables [as documented here](https://docs.smith.langchain.com/).\n",
    "\n",
    "## Building Blocks: LLMs\n",
    "\n",
    "Let's start with one of the most fundamental pieces of LangChain: the language model. LangChain includes two different types of language models: \n",
    "\n",
    "1. Text LLMs, which take a string as input and returns a string. An example of this is OpenAI's `text-davinci-003`, also known as GPT-3.\n",
    "2. Chat Models, which take a list of messages as input and return a message. An example of this is OpenAI's `gpt-4`.\n",
    "\n",
    "Because they are strings, LLM inputs and outputs are easy to visualize. So let's look at what calling a chat model directly looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"Why don't skeletons fight each other?\\n\\nThey don't have the guts!\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m }\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"Why don't skeletons fight each other?\\n\\nThey don't have the guts!\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"langchain/chat_models/openai\";\n",
    "import { HumanMessage } from \"langchain/schema\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  modelName: \"gpt-3.5-turbo-1106\",\n",
    "});\n",
    "\n",
    "await model.invoke([\n",
    "  new HumanMessage(\"Tell me a joke.\"),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we pass in an array with a single `HumanMessage` as input, and receive a single `AIMessage` as output. Messages have a `content` field containing the text value of the message, and an associated `role` that corresponds to the entity sending the message. \n",
    "\n",
    "**Note:** While this course will primarily use OpenAI's `gpt-3.5-turbo` chat model, LangChain supports models from many different providers, and you can try swapping the provided class in any of the code examples.\n",
    "\n",
    "## Building Blocks: Prompt Templates\n",
    "\n",
    "While calling models in isolation can be useful, it is often more convenient to factor out the logic behind model inputs into a reusable, parameterized component. For this purpose, LangChain includes prompt templates, which are responsible for formatting user input for later model calls. Input variables are denoted by curly braces within the template, and will be substituted into the final formatted value. \n",
    "\n",
    "Prompt templates are also useful for smoothing over some of the differences in model input types. Below, we construct a prompt template directly from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"langchain/prompts\";\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromTemplate(\n",
    "  `What are three good names for a company that makes {product}?`\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can use this prompt template to generate both string input for an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Human: What are three good names for a company that makes colorful socks?\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.format({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a message array for chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"What are three good names for a company that makes colorful socks?\"\u001b[39m,\n",
       "      additional_kwargs: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"What are three good names for a company that makes colorful socks?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.formatMessages({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that if we use the `.fromTemplate()` method, for convenience the input string gets wrapped in a `HumanMessage` and formatted as an array, which matches the input our chat model expects.\n",
    "\n",
    "We can also create a prompt template from messages directly for finer-grained control over what types of messages are passed to the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  SystemMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"You are an expert at picking company names.\"\u001b[39m,\n",
       "      additional_kwargs: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"You are an expert at picking company names.\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {}\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"What are three good names for a company that makes shiny objects?\"\u001b[39m,\n",
       "      additional_kwargs: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"What are three good names for a company that makes shiny objects?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { \n",
    "  SystemMessagePromptTemplate, \n",
    "  HumanMessagePromptTemplate \n",
    "} from \"langchain/prompts\";\n",
    "\n",
    "const promptFromMessages = ChatPromptTemplate.fromMessages([\n",
    "  SystemMessagePromptTemplate.fromTemplate(\n",
    "    \"You are an expert at picking company names.\"\n",
    "  ),\n",
    "  HumanMessagePromptTemplate.fromTemplate(\n",
    "    \"What are three good names for a company that makes {product}?\"\n",
    "  )\n",
    "]);\n",
    "\n",
    "await promptFromMessages.formatMessages({\n",
    "  product: \"shiny objects\"\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system message helps set the behavior of the assistant for many models, including OpenAI.\n",
    "\n",
    "Or for short, you can use a tuple with the role and the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  SystemMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"You are an expert at picking company names.\"\u001b[39m,\n",
       "      additional_kwargs: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"You are an expert at picking company names.\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {}\n",
       "  },\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"What are three good names for a company that makes shiny objects?\"\u001b[39m,\n",
       "      additional_kwargs: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "    content: \u001b[32m\"What are three good names for a company that makes shiny objects?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const promptFromMessages = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are an expert at picking company names.\"],\n",
    "  [\"human\", \"What are three good names for a company that makes {product}?\"]\n",
    "]);\n",
    "\n",
    "await promptFromMessages.formatMessages({\n",
    "  product: \"shiny objects\"\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we can pass these formatted values directly into a model, there's a more elegant way to use prompts and models together that we'll go over next.\n",
    "\n",
    "## Building Blocks: LangChain Expression Language\n",
    "\n",
    "LangChain Expression Language (LCEL) is a composable syntax for chaining LangChain modules together. Objects that are compatible with LCEL are called `Runnables`.\n",
    "\n",
    "We can construct a simple chain from the prompt and model we declared above like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = prompt.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the chain is the same as the first step in the sequence, which in this case is an object with a single `product` property. The prompt template is invoked with this input, then passes the properly formatted result as input into the next step of the chain, the chat model. Here's what it looks like in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"1. ChromaSock\\n2. RainbowFootwear\\n3. SpectrumSocks\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m }\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"messages\"\u001b[39m ],\n",
       "  content: \u001b[32m\"1. ChromaSock\\n2. RainbowFootwear\\n3. SpectrumSocks\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m, tool_calls: \u001b[90mundefined\u001b[39m }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.invoke({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the result is an `AIMessage` with three good names for a company that makes colorful socks.\n",
    "\n",
    "## Building Blocks: Output Parser\n",
    "\n",
    "The final consideration we'll go over in this section is formatting our output. For example, it is often easier to work with the raw string value of a chat model's output rather than an AI message. The LangChain abstraction for this is called an output parser. \n",
    "\n",
    "Below, we redefine our chain with an output parser that coerces the message output from the chat model into a string as a third and final step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StringOutputParser } from \"langchain/schema/output_parser\";\n",
    "\n",
    "const outputParser = new StringOutputParser();\n",
    "\n",
    "const nameGenerationChain = prompt.pipe(model).pipe(outputParser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, if we invoke the chain, we can see that the output is a raw string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"1. Gourmet Crumb\\n2. Delightful Desserts\\n3. Elegant Eats\"\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await nameGenerationChain.invoke({\n",
    "  product: \"fancy cookies\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a LangSmith trace of the above example [here](https://smith.langchain.com/public/9fd7689f-23c6-4ea1-8a76-fea1b5f8d5db/r).\n",
    "\n",
    "These three pieces form the core of many more complicated chains.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "One of the many advantages to using LCEL is that chains get certain methods by default. One useful one is `.stream()`, which returns output from the chain in an iterable stream. Because LLM responses often take a long time to finish, streaming is useful in situations where showing feedback quickly is important.\n",
    "\n",
    "Here's an example with the chain we just composed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      ".\n",
      " Rob\n",
      "o\n",
      "Tech\n",
      " Innov\n",
      "ations\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " Cy\n",
      "borg\n",
      " Cre\n",
      "ations\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " Fut\n",
      "ura\n",
      " Robotics\n",
      " Co\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const stream = await nameGenerationChain.stream({\n",
    "  product: \"really cool robots\",\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the `model` emits partial message chunks, and the output parser transforms those streamed chunks from the model as they are generated, resulting in string output chunks.\n",
    "\n",
    "## Batch\n",
    "\n",
    "LCEL also gives us `.batch()` out of the box too for easier concurrent operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  \u001b[32m\"1. MegaCalc Co.\\n2. TitanTech Calculators\\n3. JumboCalc Solutions\"\u001b[39m,\n",
       "  \u001b[32m\"1. Alpaca Threads\\n2. Andean Knits\\n3. Luxe Alpaca Wear\"\u001b[39m\n",
       "]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const inputs = [{\n",
    "  product: \"large calculators\"\n",
    "}, {\n",
    "  product: \"alpaca wool sweaters\"\n",
    "}]\n",
    "\n",
    "await nameGenerationChain.batch(inputs);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
