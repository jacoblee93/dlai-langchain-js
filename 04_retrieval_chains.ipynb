{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval chains\n",
    "\n",
    "Now, we'll combine the basic Expression Language building blocks, document loading and splitting, and vectorstores to create a retrieval chain that can\n",
    "perform the last two steps of the RAG process:\n",
    "\n",
    "![](./static/images/rag_diagram.png)\n",
    "\n",
    "This chain will retrieve chunks that are most similar to the input query via vector similarity search, then will present them to the LLM as context to ground the LLM's generation of a final answer.\n",
    "\n",
    "To start, let's split and load the CS229 lesson PDF transcript from earlier. For brevity, I've factored out the vectorstore initialization code into a helper function that takes arguments for chunk size and chunk overlap. We'll use bigger chunks this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { loadAndSplitChunks } from \"./lib/helpers.ts\";\n",
    "\n",
    "const splitDocs = await loadAndSplitChunks({\n",
    "  chunkSize: 1536,\n",
    "  chunkOverlap: 128,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load those docs into a vectorstore the same way we did in the previous lesson using OpenAI embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { initializeVectorstoreWithDocuments } from \"./lib/helpers.ts\";\n",
    "\n",
    "const vectorstore = await initializeVectorstoreWithDocuments({\n",
    "  documents: splitDocs,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create a `retriever` as before from that vectorstore that will fetch the documents for a given natural language query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start constructing our retrieval chain!\n",
    "\n",
    "## Document retrieval in a chain\n",
    "\n",
    "Retrievers take a string as direct input to the retriever, but we often find it convenient to have chains take an object parameter for flexibility.\n",
    "So let's start by creating a simple sequence that will take an object with a field called `question` as an input, and formats the resulting documents' page content as strings. We'll use <doc></doc> XML-esque tags to separate the contents of each for clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc>\n",
      "course information handout. So let me just say a few words about parts of these. On the \n",
      "third page, there's a section that says Online Resources.  \n",
      "Oh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \n",
      "Testing, testing. Okay, cool. Thanks.\n",
      "</doc>\n",
      "<doc>\n",
      "of this class will not be very programming intensive, although we will do some \n",
      "programming, mostly in either MATLAB or Octave. I'll say a bit more about that later.  \n",
      "I also assume familiarity with basic probability and statistics. So most undergraduate \n",
      "statistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \n",
      "assume all of you know what random variables are, that all of you know what expectation \n",
      "is, what a variance or a random variable is. And in case of some of you, it's been a while \n",
      "since you've seen some of this material. At some of the discussion sections, we'll actually \n",
      "go over some of the prerequisites, sort of as a refresher course under prerequisite class. \n",
      "I'll say a bit more about that later as well.  \n",
      "Lastly, I also assume familiarity with basic linear algebra. And again, most undergraduate \n",
      "linear algebra courses are more than enough. So if you've taken courses like Math 51, \n",
      "103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \n",
      "gonna assume that all of you know what matrixes and vectors are, that you know how to \n",
      "multiply matrices and vectors and multiply matrix and matrices, that you know what a \n",
      "matrix inverse is. If you know what an eigenvector of a matrix is, that'd be even better. \n",
      "But if you don't quite know or if you're not quite sure, that's fine, too. We'll go over it in \n",
      "the review sections.  \n",
      "So there are a couple more logistical things I should deal with in this class. One is that, as\n",
      "</doc>\n",
      "<doc>\n",
      "material that I'm teaching in the main lectures. So machine learning is a huge field, and \n",
      "there are a few extensions that we really want to teach but didn't have time in the main \n",
      "lectures for.\n",
      "</doc>\n",
      "<doc>\n",
      "same regardless of the group size, so with a larger group, you probably â€” I recommend \n",
      "trying to form a team, but it's actually totally fine to do it in a smaller group if you want.  \n",
      "Student : [Inaudible] what language [inaudible]?  \n",
      "Instructor (Andrew Ng): So let's see. There is no C programming in this class other \n",
      "than any that you may choose to do yourself in your project. So all the homeworks can be \n",
      "done in MATLAB or Octave, and let's see. And I guess the program prerequisites is more \n",
      "the ability to understand big?O notation and knowledge of what a data structure, like a \n",
      "linked list or a queue or binary treatments, more so than your knowledge of C or Java \n",
      "specifically. Yeah?  \n",
      "Student : Looking at the end semester project, I mean, what exactly will you be testing \n",
      "over there? [Inaudible]?  \n",
      "Instructor (Andrew Ng) : Of the project?  \n",
      "Student : Yeah.  \n",
      "Instructor (Andrew Ng) : Yeah, let me answer that later. In a couple of weeks, I shall \n",
      "give out a handout with guidelines for the project. But for now, we should think of the \n",
      "goal as being to do a cool piece of machine learning work that will let you experience the\n",
      "</doc>\n"
     ]
    }
   ],
   "source": [
    "import { RunnableSequence } from \"langchain/schema/runnable\";\n",
    "import { Document } from \"langchain/document\";\n",
    "\n",
    "const convertDocsToString = (documents: Document[]): string => {\n",
    "  return documents.map((document) => {\n",
    "    return `<doc>\\n${document.pageContent}\\n</doc>`\n",
    "  }).join(\"\\n\");\n",
    "};\n",
    "\n",
    "const documentRetrievalChain = RunnableSequence.from([\n",
    "  (input) => input.question,\n",
    "  retriever,\n",
    "  convertDocsToString,\n",
    "]);\n",
    "\n",
    "const results = await documentRetrievalChain.invoke({\n",
    "  question: \"What are the prerequisites for this course?\"\n",
    "});\n",
    "\n",
    "console.log(results);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like that contains the raw information! Now, let's construct a chain that synthesizes that information into a human-legible response. We'll start with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"langchain/prompts\";\n",
    "\n",
    "const TEMPLATE_STRING = `You are an experienced researcher, \n",
    "expert at interpreting and answering questions based on provided sources.\n",
    "Using the provided context, answer the user's question \n",
    "to the best of your ability using only the resources provided. \n",
    "Be verbose!\n",
    "\n",
    "<context>\n",
    "\n",
    "{context}\n",
    "\n",
    "</context>\n",
    "\n",
    "Now, answer this question using the above context:\n",
    "\n",
    "{question}`;\n",
    "\n",
    "const answerGenerationPrompt = ChatPromptTemplate.fromTemplate(TEMPLATE_STRING);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that our prompt requires an object with a `context` property as argument, while our previously defined `documentRetrievalChain` outputs a string. To make the arguments match, we use a `RunnableMap`. When a `RunnableMap` is invoked, it calls all runnables or runnable-like objects that it has as properties in parallel, invoking each with the input to the map. Then outputs an object whose properties are the results of those calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  question: \u001b[32m\"What are the prerequisites for this course?\"\u001b[39m,\n",
       "  context: \u001b[32m\"<doc>\\n\"\u001b[39m +\n",
       "    \u001b[32m\"course information handout. So let me just say a few words about parts of these. On the \\n\"\u001b[39m +\n",
       "    \u001b[32m\"third\"\u001b[39m... 3063 more characters\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { RunnableMap } from \"langchain/schema/runnable\";\n",
    "\n",
    "const runnableMap = RunnableMap.from({\n",
    "  context: documentRetrievalChain,\n",
    "  question: (input) => input.question,\n",
    "});\n",
    "\n",
    "await runnableMap.invoke({ question: \"What are the prerequisites for this course?\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, `documentRetrievalChain` is invoked with the object `{ question: \"What are the prerequisites for this course?\" }`, which results in the `documentRetrievalChain`'s output as a property names `context`. \n",
    "\n",
    "## Augmented generation\n",
    "\n",
    "And that's the format we need to pass to our prompt! Let's see what this looks like with our document retrieval sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"langchain/chat_models/openai\";\n",
    "import { StringOutputParser } from \"langchain/schema/output_parser\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  modelName: \"gpt-3.5-turbo-1106\",\n",
    "});\n",
    "\n",
    "const retrievalChain = RunnableSequence.from([\n",
    "  {\n",
    "    context: documentRetrievalChain,\n",
    "    question: (input) => input.question,\n",
    "  },\n",
    "  answerGenerationPrompt,\n",
    "  model,\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note: \n",
    "\n",
    "- In the `RunnableSequence.from` method, objects are automatically coerced into `RunnableMap`s, so there is no need to use the initializer method.\n",
    "- Because we want to pass `question` into both the `documentRetrievalChain` to fetch relevant documents as well as the `answerGenerationPrompt`, we add a second property to the `RunnableMap` called `question` that extracts just the `question` field from the original input to the map. This means that the answer generation prompt gets an object with both properties.\n",
    "\n",
    "Because this pattern is so common, there is a helper called `RunnablePassthrough.assign()` that assigns new values to a `RunnableMap` while passing through through all existing properties. You could thus rewrite the above like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnablePassthrough } from \"langchain/runnables\";\n",
    "\n",
    "const retrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationPrompt,\n",
    "  model,\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try it end-to-end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the course information provided in the context, the instructor assumes that students are familiar with basic probability and statistics, as well as basic linear algebra. For probability and statistics, it is mentioned that undergraduate statistics classes, like Stat 116 at Stanford, would provide sufficient preparation. Additionally, for linear algebra, undergraduate courses such as Math 51, Math 103, Math 113, or CS205 at Stanford are considered adequate prerequisites. The instructor mentions that students should be familiar with concepts such as random variables, expectation, variance, matrices, vectors, matrix multiplication, matrix inversion, and eigenvectors. However, the instructor also acknowledges that some students may need a refresher on these topics, and review sessions will be held to cover the prerequisites as needed. Overall, the prerequisites for this course include a basic understanding of probability, statistics, and linear algebra concepts.\n"
     ]
    }
   ],
   "source": [
    "const answer = await retrievalChain.invoke({\n",
    "  question: \"What are the prerequisites for this course?\"\n",
    "});\n",
    "\n",
    "console.log(answer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! That looks pretty good.\n",
    "\n",
    "But what if we want to ask a followup question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I am unable to identify a specific question or request for a list that can be answered in bullet point form. The context primarily consists of a course information handout and a lecture on machine learning, but there is no explicit question or request present. If there is a specific question or request you would like me to address, please provide it and I will do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "const followupAnswer = await retrievalChain.invoke({\n",
    "  question: \"Can you list them in bullet point form?\"\n",
    "});\n",
    "\n",
    "console.log(followupAnswer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didn't do so well there!\n",
    "\n",
    "This occurs because LLMs do not have an innate sense of memory, and since we're not passing in any chat history as context, the LLM doesn't know what \"them\" referes to. We can update our prompt to take chat history into account as well, but we have a more fundamental problem: our vectorstore needs to return relevant documents too. Here's what happens if we try to query our vectorstore with the current followup question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc>\n",
      "course information handout. So let me just say a few words about parts of these. On the \n",
      "third page, there's a section that says Online Resources.  \n",
      "Oh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \n",
      "Testing, testing. Okay, cool. Thanks.\n",
      "</doc>\n",
      "<doc>\n",
      "into four major sections. We're gonna talk about four major topics in this class, the first \n",
      "of which is supervised learning. So let me give you an example of that.  \n",
      "So suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \n",
      "actually collected a data set for me last week to use in the example later. But suppose that \n",
      "you go to collect statistics about how much houses cost in a certain geographic area. And \n",
      "Dan, the TA, collected data from housing prices in Portland, Oregon. So what you can do \n",
      "is let's say plot the square footage of the house against the list price of the house, right, so \n",
      "you collect data on a bunch of houses. And let's say you get a data set like this with \n",
      "houses of different sizes that are listed for different amounts of money.  \n",
      "Now, let's say that I'm trying to sell a house in the same area as Portland, Oregon as \n",
      "where the data comes from. Let's say I have a house that's this size in square footage, and \n",
      "I want an algorithm to tell me about how much should I expect my house to sell for. So \n",
      "there are lots of ways to do this, and some of you may have seen elements of what I'm \n",
      "about to say before.  \n",
      "So one thing you could do is look at this data and maybe put a straight line to it. And then \n",
      "if this is my house, you may then look at the straight line and predict that my house is \n",
      "gonna go for about that much money, right? There are other decisions that we can make, \n",
      "which we'll talk about later, which is, well, what if I don't wanna put a straight line?\n",
      "</doc>\n",
      "<doc>\n",
      "joys of machine learning firsthand and really try to think about doing a publishable piece \n",
      "of work.  \n",
      "So many students will try to build a cool machine learning application. That's probably \n",
      "the most common project. Some students will try to improve state-of-the-art machine \n",
      "learning. Some of those projects are also very successful. It's a little bit harder to do. And \n",
      "there's also a smaller minority of students that will sometimes try to prove â€” develop the \n",
      "theory of machine learning further or try to prove theorems about machine learning. So \n",
      "they're usually great projects of all of those types with applications and machine learning \n",
      "being the most common. Anything else? Okay, cool.  \n",
      "So that was it for logistics. Let's talk about learning algorithms. So can I have the laptop \n",
      "display, please, or the projector? Actually, could you lower the big screen? Cool. This is \n",
      "amazing customer service. Thank you. I see. Okay, cool. Okay. No, that's fine. I see. \n",
      "Okay. That's cool. Thanks. Okay.  \n",
      "Big screen isn't working today, but I hope you can read things on the smaller screens out \n",
      "there. Actually, [inaudible] I think this room just got a new projector that â€” someone \n",
      "sent you an excited email â€” was it just on Friday? â€” saying we just got a new projector \n",
      "and they said 4,000-to-1 something or other brightness ratio. I don't know. Someone was \n",
      "very excited about the new projector in this room, but I guess we'll see that in operation \n",
      "on Wednesday.\n",
      "</doc>\n",
      "<doc>\n",
      "which we'll talk about later, which is, well, what if I don't wanna put a straight line? \n",
      "Maybe I should put a quadratic function to it. Maybe that fits the data a little bit better. \n",
      "You notice if you do that, the price of my house goes up a bit, so that'd be nice.\n",
      "</doc>\n"
     ]
    }
   ],
   "source": [
    "const docs = await documentRetrievalChain.invoke({\n",
    "  question: \"Can you list them in bullet point form?\"\n",
    "});\n",
    "\n",
    "console.log(docs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we don't get anything relevant to the prerequisites of CS229.\n",
    "\n",
    "## Adding history\n",
    "\n",
    "The solution is to dereference the user's question into a rephrased standalone question. How? With an LLM of course!\n",
    "\n",
    "First, we'll construct a new prompt with a `MessagesPlaceholder` where we can later inject or pass chat history messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesPlaceholder } from \"langchain/prompts\";\n",
    "\n",
    "const REPHRASE_QUESTION_SYSTEM_TEMPLATE = \n",
    "  `Given the following conversation and a follow up question, \n",
    "rephrase the follow up question to be a standalone question.`;\n",
    "\n",
    "const rephraseQuestionChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", REPHRASE_QUESTION_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\n",
    "    \"human\", \n",
    "    \"Rephrase the following question as a standalone question:\\n{question}\"\n",
    "  ],\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we'll create a simple chain that uses this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "const rephraseQuestionChain = RunnableSequence.from([\n",
    "  rephraseQuestionChainPrompt,\n",
    "  new ChatOpenAI({ temperature: 0.1, modelName: \"gpt-3.5-turbo-1106\" }),\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running this chain on our followup question. Note that `MessagesPlaceholder` is itself a parameter in our prompt that accepts a list of chat messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prerequisites for this course include familiarity with basic probability and statistics, as well as basic linear algebra. The instructor assumes that students are already familiar with concepts such as random variables, expectation, variance, matrices, vectors, matrix multiplication, and matrix inverse. It is also mentioned that undergraduate statistics classes such as Stat 116 and undergraduate linear algebra courses like Math 51, 103, Math 113, or CS205 taught at Stanford would provide sufficient background for this course. Additionally, the ability to understand big O notation and knowledge of data structures such as linked lists, queues, and binary treatments is considered more important than specific programming language knowledge such as C or Java.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Could you please list them in bullet point form?\"\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { HumanMessage, AIMessage } from \"langchain/schema\";\n",
    "\n",
    "const originalQuestion = \"What are the prerequisites for this course?\";\n",
    "\n",
    "const originalAnswer = await retrievalChain.invoke({\n",
    "  question: originalQuestion\n",
    "});\n",
    "\n",
    "console.log(originalAnswer);\n",
    "\n",
    "const chatHistory = [\n",
    "  new HumanMessage(originalQuestion),\n",
    "  new AIMessage(originalAnswer),\n",
    "];\n",
    "\n",
    "await rephraseQuestionChain.invoke({\n",
    "  question: \"Can you list them in bullet point form?\",\n",
    "  history: chatHistory,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! That makes sense on its own. Now, let's put it all together with a new chain!\n",
    "\n",
    "First, here's our document retrieval and formatting chain again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "const convertDocsToString = (documents: Document[]): string => {\n",
    "  return documents.map((document) => `<doc>\\n${document.pageContent}\\n</doc>`).join(\"\\n\");\n",
    "};\n",
    "\n",
    "const documentRetrievalChain = RunnableSequence.from([\n",
    "  (input) => input.standalone_question,\n",
    "  retriever,\n",
    "  convertDocsToString,\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's redefine our answer generation prompt to also have a `MessagesPlaceholder` for history messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are an experienced researcher, \n",
    "expert at interpreting and answering questions based on provided sources.\n",
    "Using the below provided context and chat history, \n",
    "answer the user's question to the best of \n",
    "your ability \n",
    "using only the resources provided. Be verbose!\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>`;\n",
    "\n",
    "const answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", ANSWER_CHAIN_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\n",
    "    \"human\", \n",
    "    \"Now, answer this question using the previous context and chat history:\\n{standalone_question}\"\n",
    "  ]\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt requires three inputs: `context` from the retriever, a `standalone_question` from the rephrasing chain, and an array of chat messages as `history`. We can invoke it with dummy values to get a sense of what's needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InputFormatError",
     "evalue": "Error: Field \"history\" in prompt uses a MessagesPlaceholder, which expects an array of BaseMessages as an input value. Received: [\n  [\n    \"human\",\n    \"How are you?\"\n  ],\n  [\n    \"ai\",\n    \"Fine, thank you!\"\n  ]\n]",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "InputFormatError: Error: Field \"history\" in prompt uses a MessagesPlaceholder, which expects an array of BaseMessages as an input value. Received: [",
      "  [",
      "    \"human\",",
      "    \"How are you?\"",
      "  ],",
      "  [",
      "    \"ai\",",
      "    \"Fine, thank you!\"",
      "  ]",
      "]",
      "    at MessagesPlaceholder.validateInputOrThrow (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.0.11-rc.1/dist/prompts/chat.js:74:27)",
      "    at MessagesPlaceholder.formatMessages (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.0.11-rc.1/dist/prompts/chat.js:81:14)",
      "    at ChatPromptTemplate.formatMessages (file:///Users/jacoblee/Library/Caches/deno/npm/registry.npmjs.org/@langchain/core/0.0.11-rc.1/dist/prompts/chat.js:355:53)",
      "    at async <anonymous>:3:1"
     ]
    }
   ],
   "source": [
    "import { HumanMessage, AIMessage } from \"langchain/schema\";\n",
    "await answerGenerationChainPrompt.formatMessages({\n",
    "  context: \"fake retrieved content\",\n",
    "  standalone_question: \"Why is the sky blue?\",\n",
    "  history: [\n",
    "    new HumanMessage(\"How are you?\"),\n",
    "    new AIMessage(\"Fine, thank you!\")\n",
    "  ]\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's assemble our conversation-capable retrieval chain by passing history cleanly through until the final generation prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const conversationalRetrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    standalone_question: rephraseQuestionChain,\n",
    "  }),\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationChainPrompt,\n",
    "  new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }),\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could pass history back and forth here, but we can instead streamline chat history tracking and sessions using a `MessageHistory` object, then wrap our chain in a manager that will automatically update the history, called `RunnableWithMessageHistory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableWithMessageHistory } from \"langchain/runnables\";\n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "\n",
    "const messageHistory = new ChatMessageHistory();\n",
    "\n",
    "const finalRetrievalChain = new RunnableWithMessageHistory({\n",
    "  runnable: conversationalRetrievalChain,\n",
    "  getMessageHistory: (_sessionId) => messageHistory,\n",
    "  historyMessagesKey: \"history\",\n",
    "  inputMessagesKey: \"question\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RunnableWithMessageHistory` class wraps a runnable and automatically adds an additional property given by `historyMessagesKey` as the runnable's input. After it is invoked, it also updates the chat history with the value passed as `inputMessagesKey`, in this case, `question`.\n",
    "\n",
    "`getMessageHistory` is a function that returns a new chat history object based on the passed session id. In the above demo case, we reuse the same history object for all calls, but in production environments, you'll want to assign a new object for each session to avoid mixing conversation histories up.\n",
    "\n",
    "Let's try out the finished version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Familiarity with basic probability and statistics\n",
      "- Knowledge of random variables, expectation, variance, and probability\n",
      "- Completion of an undergraduate statistics class, such as Stat 116 at Stanford\n",
      "- Familiarity with basic linear algebra\n",
      "- Understanding of matrices, vectors, matrix multiplication, matrix inverse, and eigenvectors\n",
      "- Completion of an undergraduate linear algebra course, such as Math 51, 103, Math 113, or CS205 at Stanford\n",
      "- Basic programming skills, preferably in MATLAB or Octave\n",
      "- Familiarity with big-O notation and basic computer skills\n"
     ]
    }
   ],
   "source": [
    "const originalQuestion = \"What are the prerequisites for this course?\";\n",
    "\n",
    "const originalAnswer = await finalRetrievalChain.invoke({\n",
    "  question: originalQuestion,\n",
    "}, {\n",
    "  configurable: { sessionId: \"test\" }\n",
    "});\n",
    "\n",
    "const finalResult = await finalRetrievalChain.invoke({\n",
    "  question: \"Can you list them in bullet point form?\",\n",
    "}, {\n",
    "  configurable: { sessionId: \"test\" }\n",
    "});\n",
    "\n",
    "console.log(finalResult);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peruse this trace for an interactive example of the internals of the chain: https://smith.langchain.com/public/601c9879-54f3-4b5e-a09b-7b51d3c96757/r\n",
    "\n",
    "Retrieval is a very deep topic, and there's no one-size fits all approach for loading, splitting, and querying your data. We encourage you to modify the above prompts and parameters for different models and data types.\n",
    "\n",
    "In the final section, we'll show how to put this retrieval chain into production, including some interactions with web APIs and streaming intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
