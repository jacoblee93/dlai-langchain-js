{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Now that we've designed a simple retrieval chain, let's look at what it would take to productionize and deploy it as a streaming chat endpoint!\n",
    "\n",
    "We'll go over the interaction with native web primitives like `fetch` and `Response`, as well as show how to utilize different chat sessions.\n",
    "\n",
    "We'll pick up where we left off in the last lesson with loading and splitting our CS229 transcript into a vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  loadAndSplitChunks, \n",
    "  initializeVectorstoreWithDocuments \n",
    "} from \"./lib/helpers.ts\";\n",
    "\n",
    "const splitDocs = await loadAndSplitChunks({\n",
    "  chunkSize: 1536,\n",
    "  chunkOverlap: 128,\n",
    "});\n",
    "\n",
    "const vectorstore = await initializeVectorstoreWithDocuments({\n",
    "  documents: splitDocs,\n",
    "});\n",
    "\n",
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pieces of our conversational retrieval chain together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  createDocumentRetrievalChain, \n",
    "  createRephraseQuestionChain \n",
    "} from \"./lib/helpers.ts\";\n",
    "\n",
    "const documentRetrievalChain = createDocumentRetrievalChain();\n",
    "const rephraseQuestionChain = createRephraseQuestionChain();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\n",
    "\n",
    "const ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are an experienced researcher,\n",
    "expert at interpreting and answering questions based on provided sources.\n",
    "Using the below provided context and chat history, \n",
    "answer the user's question to the best of your ability\n",
    "using only the resources provided. Be verbose!\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>`;\n",
    "\n",
    "const HUMAN_MESSAGE_TEMPLATE = \n",
    "  `Now, answer this question using the previous context and chat history:\n",
    "  \n",
    "  {standalone_question}`;\n",
    "\n",
    "const answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", ANSWER_CHAIN_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\n",
    "    \"human\", \n",
    "    HUMAN_MESSAGE_TEMPLATE\n",
    "  ]\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we assemble all the pieces together, let's note that the native web `Response` objects used to return data in popular frameworks like Next.js accept a `ReadableStream` the emits bytes directly. Previously, our chain outputted string chunks directly using `StringOutputParser`, but it would be convenient to be able to directly stream so that we could pass our LangChain stream directly to the response.\n",
    "\n",
    "Fortunately, LangChain provides an `HttpResponseOutputParser` that parses chat output into chunks of bytes that match either `text/plain` or `text/event-stream` content types! To use it, let's construct our conversational retrieval chain as before, but skip the final `StringOutputParser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  RunnablePassthrough, \n",
    "  RunnableSequence \n",
    "} from \"langchain/runnables\";\n",
    "import { ChatOpenAI } from \"langchain/chat_models/openai\";\n",
    "\n",
    "const conversationalRetrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    standalone_question: rephraseQuestionChain,\n",
    "  }),\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationChainPrompt,\n",
    "  new ChatOpenAI({ modelName: \"gpt-3.5-turbo-1106\" }),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create an `HttpResponseOutputParser` and pipe the `RunnableWithMessageHistory` into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HttpResponseOutputParser } from \"langchain/output_parsers\";\n",
    "import { RunnableWithMessageHistory } from \"langchain/runnables\"; \n",
    "import { ChatMessageHistory } from \"langchain/stores/messages/in_memory\";\n",
    "\n",
    "// \"text/event-stream\" is also supported\n",
    "const httpResponseOutputParser = new HttpResponseOutputParser({\n",
    "  contentType: \"text/plain\"\n",
    "});\n",
    "\n",
    "const messageHistoryMap = new Map();\n",
    "\n",
    "const finalRetrievalChain = new RunnableWithMessageHistory({\n",
    "  runnable: conversationalRetrievalChain,\n",
    "  // Mention where sessionId gets passed from (parameter to our endpoint)\n",
    "  getMessageHistory: (sessionId) => {\n",
    "    if (sessionId in messageHistoryMap) {\n",
    "      return messageHistoryMap.get(sessionId);\n",
    "    }\n",
    "    const newChatSessionHistory = new ChatMessageHistory();\n",
    "    messageHistoryMap.set(sessionId, newChatSessionHistory);\n",
    "    return newChatSessionHistory;\n",
    "  },\n",
    "  inputMessagesKey: \"question\",\n",
    "  historyMessagesKey: \"history\",\n",
    "}).pipe(httpResponseOutputParser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we don't put the `HttpResponseOutputParser` directly in the `conversationalRetrievalChain` is because `RunnableWithMessageHistory` will store the aggregated output of its runnable in the `ChatMessageHistory`, and requires either a string or a `ChatMessage` to be the final output rather than bytes.\n",
    "\n",
    "You might also notice that our `getMessageHistory` function creates a new `ChatMessageHistory` object based on the passed `sessionId` instead of reusing the same one as before. This allows us to assign `sessionId`s properly to individual conversations and load them as requests come in later. For more advanced persistence, you'll want to use a integration to store these histories.\n",
    "\n",
    "Great! Let's set up a simple server with a handler that calls our chain and see if we can get a streaming response. We'll populate the input question and the session id from the body parameters. Since this notebook is written in Deno, we use a Deno built-in HTTP method, but this general concept is shared by many JS frameworks.\n",
    "\n",
    "Also, in a true production deployment, you'd likely want to set up authentication/input validation via some middleware, but we'll skip that for simplicity for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP server is running! Access it at: http://localhost:8080/\n",
      "Listening on http://localhost:8080/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  finished: Promise { \u001b[36m<pending>\u001b[39m },\n",
       "  shutdown: \u001b[36m[AsyncFunction: shutdown]\u001b[39m,\n",
       "  ref: \u001b[36m[Function: ref]\u001b[39m,\n",
       "  unref: \u001b[36m[Function: unref]\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const port = 8080;\n",
    "\n",
    "const handler = async (request: Request): Response => {\n",
    "  const body = await request.json();\n",
    "  const stream = await finalRetrievalChain.stream({\n",
    "    question: body.question\n",
    "  }, { configurable: { sessionId: body.session_id } });\n",
    "\n",
    "  return new Response(stream, { \n",
    "    status: 200,\n",
    "    headers: {\n",
    "      \"Content-Type\": \"text/plain\"\n",
    "    },\n",
    "  });\n",
    "};\n",
    "\n",
    "console.log(`HTTP server is running! Access it at: http://localhost:${port}/`);\n",
    "Deno.serve({ port }, handler);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a quick helper function to make handling the response stream in the client a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const decoder = new TextDecoder();\n",
    "\n",
    "// readChunks() reads from the provided reader and yields the results into an async iterable\n",
    "function readChunks(reader) {\n",
    "  return {\n",
    "    async* [Symbol.asyncIterator]() {\n",
    "      let readResult = await reader.read();\n",
    "      while (!readResult.done) {\n",
    "        yield decoder.decode(readResult.value);\n",
    "        readResult = await reader.read();\n",
    "      }\n",
    "    },\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try calling our endpoint!\n",
    "\n",
    "We use a sleep function at the end due to the limitations of running a server within a notebook - we want to make sure the request finishes before the cell stop execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: The prerequ\n",
      "CHUNK: isites for this \n",
      "CHUNK: course, as me\n",
      "CHUNK: ntioned by the instructor, are\n",
      "CHUNK:  as follows:\n",
      "\n",
      "1. Familia\n",
      "CHUNK: rity with bas\n",
      "CHUNK: ic probab\n",
      "CHUNK: ility and statis\n",
      "CHUNK: tics, which is \n",
      "CHUNK: assumed to be cov\n",
      "CHUNK: ered in most unde\n",
      "CHUNK: rgraduate statistics\n",
      "CHUNK:  classes l\n",
      "CHUNK: ike Stat 116 at Stanfor\n",
      "CHUNK: d University.\n",
      "2.\n",
      "CHUNK:  Basic familiarity with linea\n",
      "CHUNK: r algebra\n",
      "CHUNK: , which can be ob\n",
      "CHUNK: tained through\n",
      "CHUNK:  undergraduate linear algebra courses such as \n",
      "CHUNK: Math 51, 1\n",
      "CHUNK: 03, Math 113, or CS2\n",
      "CHUNK: 05 at Stanford. The \n",
      "CHUNK: expectation is that st\n",
      "CHUNK: udents know a\n",
      "CHUNK: bout matrices, ve\n",
      "CHUNK: ctors, matrix multiplica\n",
      "CHUNK: tion, matrix inverses, and preferably eigen\n",
      "CHUNK: vectors, but there will\n",
      "CHUNK:  be review sections\n",
      "CHUNK:  to assist those\n",
      "CHUNK:  who need i\n",
      "CHUNK: t.\n",
      "\n",
      "The course\n",
      "CHUNK:  is also said to be n\n",
      "CHUNK: ot very programmin\n",
      "CHUNK: g intensive, although so\n",
      "CHUNK: me programming will be in\n",
      "CHUNK: volved,\n",
      "CHUNK:  mostly in MATLAB or Octa\n",
      "CHUNK: ve. Students are \n",
      "CHUNK: not required to have extensive\n",
      "CHUNK:  programming knowledge in langu\n",
      "CHUNK: ages such as C or Java,\n",
      "CHUNK:  but they should be able to \n",
      "CHUNK: understand big O notation \n",
      "CHUNK: and have knowledge of data \n",
      "CHUNK: structures like linked lists\n",
      "CHUNK: , queues, and binary tre\n",
      "CHUNK: es. The end semester\n",
      "CHUNK:  project will involve \n",
      "CHUNK: a cool piece of mac\n",
      "CHUNK: hine learning work, and \n",
      "CHUNK: the specifics of w\n",
      "CHUNK: hat will be tested in the p\n",
      "CHUNK: roject will be communicated in a handout with guidelines a couple of weeks later in the course.\n",
      "\n",
      "In summary, the main prerequisites listed are familiarity with basic probability and statistics, and basic understanding of linear algebra, as well as some knowledge of data structures and big O notation.\n"
     ]
    }
   ],
   "source": [
    "const sleep = async () => {\n",
    "  return new Promise((resolve) => setTimeout(resolve, 500));\n",
    "}\n",
    "\n",
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What are the prerequisites for this course?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get a streamed string response.\n",
    "\n",
    "Now, let's test the memory by asking a followup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: Certain\n",
      "CHUNK: ly! Based on the provi\n",
      "CHUNK: ded context and chat\n",
      "CHUNK:  history, the bulle\n",
      "CHUNK: t point f\n",
      "CHUNK: ormat for the response \n",
      "CHUNK: would be as \n",
      "CHUNK: follows:\n",
      "\n",
      "- Stude\n",
      "CHUNK: nts' Projec\n",
      "CHUNK: ts in Machine Learning\n",
      "CHUNK: :\n",
      "    - Build\n",
      "CHUNK: ing cool m\n",
      "CHUNK: achine learning applic\n",
      "CHUNK: ations\n",
      "    - Improving sta\n",
      "CHUNK: te-of-the-ar\n",
      "CHUNK: t machine learning\n",
      "    - Developing \n",
      "CHUNK: the theory of\n",
      "CHUNK:  machine learning further or\n",
      "CHUNK:  proving theorems abou\n",
      "CHUNK: t machine learning\n",
      "\n",
      "- Audienc\n",
      "CHUNK: e Diversity in the Class\n",
      "CHUNK: :\n",
      "    - Various backgrounds in\n",
      "CHUNK: cluding iCME, Civi, S\n",
      "CHUNK: ynthesis,\n",
      "CHUNK:  Aero/astro, MSN\n",
      "CHUNK: E, Endo, and i\n",
      "CHUNK: ndustry\n",
      "    - Andrew Ng e\n",
      "CHUNK: xpresses that \n",
      "CHUNK: the diverse a\n",
      "CHUNK: udience makes the class fun \n",
      "CHUNK: to teach and to\n",
      "CHUNK:  be in\n",
      "\n",
      "- Use of \n",
      "CHUNK: Learning Algorithms:\n",
      "    - \n",
      "CHUNK: Andrew Ng showcases \n",
      "CHUNK: applications of learning algorithms throu\n",
      "CHUNK: gh examples of \n",
      "CHUNK: teaching a car to dr\n",
      "CHUNK: ive off roa\n",
      "CHUNK: ds and robots to accomplish \n",
      "CHUNK: complex tasks\n",
      "    - Em\n",
      "CHUNK: phasizes the abi\n",
      "CHUNK: lity of learning a\n",
      "CHUNK: lgorithms to enable \n",
      "CHUNK: robots to perform amazing tasks without extensive hand-coding\n",
      "\n",
      "- Encouragement for Students:\n",
      "    - Encouragement to form project partners and study groups\n",
      "    - Emphasizes the importance of these partnerships for the project work in the course\n",
      "\n",
      "I hope this format is helpful!\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"Can you list them in bullet point format?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Let's try again with a different `sessionId`. We expect to see a wholly new loaded conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: Based on t\n",
      "CHUNK: he context \n",
      "CHUNK: and chat history provide\n",
      "CHUNK: d, it seems t\n",
      "CHUNK: hat you haven't expli\n",
      "CHUNK: citly asked \n",
      "CHUNK: a question yet. T\n",
      "CHUNK: he context mainly\n",
      "CHUNK:  consists of \n",
      "CHUNK: a lecture t\n",
      "CHUNK: ranscript,\n",
      "CHUNK:  some course-r\n",
      "CHUNK: elated announcem\n",
      "CHUNK: ents, and a brief \n",
      "CHUNK: dialogue between the i\n",
      "CHUNK: nstructor (Andrew Ng) and the students, but there's no specific question posed by you within this information.\n",
      "\n",
      "If you could provide the specific question you'd like me to address, I'd be more than happy to help answer it using the provided context and resources.\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What did I just ask you?\",\n",
    "    session_id: \"2\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entirely new session! While the current version of the Deno kernel can't currently render a frontend in the notebook, you could update a frontend component with the content of the stream to create a responsive chat experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
