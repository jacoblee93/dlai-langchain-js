{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Now that we've designed a simple retrieval chain, let's look at what it would take to deploy it as a streaming chat endpoint!\n",
    "\n",
    "We'll pick up where we left off in the last lesson with loading and splitting our CS229 transcript into a vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"npm:dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Peer dependency\n",
    "import * as parse from \"npm:pdf-parse\";\n",
    "import { PDFLoader } from \"npm:langchain@0.0.202/document_loaders/fs/pdf\";\n",
    "import { RecursiveCharacterTextSplitter } from \"npm:langchain@0.0.202/text_splitter\";\n",
    "import { MemoryVectorStore } from \"npm:langchain@0.0.202/vectorstores/memory\";\n",
    "import { OpenAIEmbeddings } from \"npm:langchain@0.0.202/embeddings/openai\";\n",
    "\n",
    "const loader = new PDFLoader(\"./static/docs/MachineLearning-Lecture01.pdf\");\n",
    "\n",
    "const rawCS229Docs = await loader.load();\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 1536,\n",
    "  chunkOverlap: 128,\n",
    "});\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(rawCS229Docs);\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings();\n",
    "\n",
    "const vectorstore = new MemoryVectorStore(embeddings);\n",
    "\n",
    "await vectorstore.addDocuments(splitDocs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a retriever from the vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, putting the pieces of our conversational retrieval chain together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableSequence } from \"npm:langchain@0.0.202/runnables\";\n",
    "\n",
    "const convertDocsToString = (documents: Document[]): string => {\n",
    "  return documents.map((document) => `<doc>\\n${document.pageContent}\\n</doc>`).join(\"\\n\");\n",
    "};\n",
    "\n",
    "const documentRetrievalChain = RunnableSequence.from([\n",
    "  (input) => input.standalone_question,\n",
    "  retriever,\n",
    "  convertDocsToString,\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"npm:langchain@0.0.202/chat_models/openai\";\n",
    "import { StringOutputParser } from \"npm:langchain@0.0.202/schema/output_parser\";\n",
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"npm:langchain@0.0.202/prompts\";\n",
    "\n",
    "const REPHRASE_QUESTION_SYSTEM_TEMPLATE = `Using the provided chat history as context, rephrase the following question to be a standalone question that has no external references.\n",
    "\n",
    "Do not respond with anything other than a rephrased standalone question.`;\n",
    "\n",
    "const rephraseQuestionChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", REPHRASE_QUESTION_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\"human\", \"Now, answer the following question:\\n{question}\"],\n",
    "]);\n",
    "\n",
    "const rephraseQuestionChain = RunnableSequence.from([\n",
    "  rephraseQuestionChainPrompt,\n",
    "  new ChatOpenAI({ temperature: 0, modelName: \"gpt-3.5-turbo-1106\" }),\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are an experienced researcher, expert at interpreting and answering questions based on provided sources.\n",
    "Using the below provided context and chat history, answer the user's question to the best of your ability using only the resources provided. Be concise!\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>`;\n",
    "\n",
    "const answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", ANSWER_CHAIN_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\"human\", \"Now, answer this question using the previous context and chat history:\\n{standalone_question}\"]\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we assemble all the pieces together, let's note that web `Response` objects that you return in popular frameworks like Next.js accept a `ReadableStream` the emits bytes directly. Previously, our chain outputted string chunks directly using `StringOutputParser`, but it would be convenient to be able to directly stream so that we could pass our LangChain stream directly to the response.\n",
    "\n",
    "Fortunately, LangChain provides an `HttpResponseOutputParser` that parses chat output into chunks of bytes that match either `text/plain` or `text/event-stream` content types! To use it, let's construct our conversational retrieval chain as before, but skip the final `StringOutputParser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnablePassthrough } from \"npm:langchain@0.0.202/runnables\";\n",
    "\n",
    "const conversationalRetrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    standalone_question: rephraseQuestionChain,\n",
    "  }),\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationChainPrompt,\n",
    "  new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create an `HttpResponseOutputParser` and pipe the `RunnableWithMessageHistory` into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HttpResponseOutputParser } from \"npm:langchain@0.0.202/output_parsers\";\n",
    "import { RunnableWithMessageHistory } from \"npm:langchain@0.0.202/runnables\"; \n",
    "import { ChatMessageHistory } from \"npm:langchain@0.0.202/memory\";\n",
    "\n",
    "// \"text/event-stream\" is also supported\n",
    "const httpResponseOutputParser = new HttpResponseOutputParser({\n",
    "  contentType: \"text/plain\"\n",
    "});\n",
    "\n",
    "const messageHistory = new ChatMessageHistory();\n",
    "\n",
    "const conversationalRetrievalChainWithHistory = new RunnableWithMessageHistory({\n",
    "  runnable: conversationalRetrievalChain,\n",
    "  // Mention where sessionId gets passed from (parameter to our endpoint)\n",
    "  getMessageHistory: (_sessionId) => messageHistory,\n",
    "  inputMessagesKey: \"question\",\n",
    "  historyMessagesKey: \"history\",\n",
    "}).pipe(httpResponseOutputParser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we don't put the `HttpResponseOutputParser` directly in the `conversationalRetrievalChain` is because `RunnableWithMessageHistory` will store the aggregated output of its runnable in the `ChatMessageHistory`, and requires either a string or a `ChatMessage` to be the final output rather than bytes.\n",
    "\n",
    "You might also notice that our `getMessageHistory` function creates a new `RedisMessageHistory` object instead of reusing an in memory `ChatMessageHistory` as before. This allows us to assign `sessionId`s properly to individual conversations and load them as requests come in later.\n",
    "\n",
    "Great! Let's set up a simple server with a handler that calls our chain and see if we can get a streaming response. We'll populate the input question and the session id from the body paramters. Since this notebook is written in Deno, we use a Deno built-in HTTP method, but this general concept is shared by many JS frameworks.\n",
    "\n",
    "Also, in a true production deployment, you'd likely want to set up authentication/input validation, but we'll skip that for simplicity for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP server is running! Access it at: http://localhost:8080/\n",
      "Listening on http://localhost:8080/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  finished: Promise { \u001b[36m<pending>\u001b[39m },\n",
       "  shutdown: \u001b[36m[AsyncFunction: shutdown]\u001b[39m,\n",
       "  ref: \u001b[36m[Function: ref]\u001b[39m,\n",
       "  unref: \u001b[36m[Function: unref]\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const port = 8080;\n",
    "\n",
    "const handler = async (request: Request): Response => {\n",
    "  const body = await request.json();\n",
    "  const stream = await conversationalRetrievalChainWithHistory.stream({\n",
    "    question: body.question\n",
    "  }, { configurable: { sessionId: body.session_id } });\n",
    "\n",
    "  return new Response(stream, { \n",
    "    status: 200,\n",
    "    headers: {\n",
    "      \"Content-Type\": \"text/plain\"\n",
    "    }\n",
    "  });\n",
    "};\n",
    "\n",
    "console.log(`HTTP server is running! Access it at: http://localhost:${port}/`);\n",
    "Deno.serve({ port }, handler);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a quick helper function to make handling the response stream in the client a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "const decoder = new TextDecoder();\n",
    "\n",
    "// readChunks() reads from the provided reader and yields the results into an async iterable\n",
    "function readChunks(reader) {\n",
    "  return {\n",
    "    async* [Symbol.asyncIterator]() {\n",
    "      let readResult = await reader.read();\n",
    "      while (!readResult.done) {\n",
    "        yield decoder.decode(readResult.value);\n",
    "        readResult = await reader.read();\n",
    "      }\n",
    "    },\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try calling our endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: The req\n",
      "CHUNK: uirements for\n",
      "CHUNK:  this course \n",
      "CHUNK: are familiarity wit\n",
      "CHUNK: h basic probabil\n",
      "CHUNK: ity and statistics, basic linear algebra, and some programming experience in MATLAB or Octave. Familiarity with random variables, expectation, variance, matrices, vectors, matrix multiplication, matrix inverse, and eigenvectors is also beneficial but not mandatory.\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What are the prerequisites for this course?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get a streamed string response that we could render in a frontend web component!\n",
    "\n",
    "Now, let's test the memory by asking a followup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "error sending request for url (http://localhost:8080/): error trying to connect: tcp connect error: Connection refused (os error 61)",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: error sending request for url (http://localhost:8080/): error trying to connect: tcp connect error: Connection refused (os error 61)",
      "    at async mainFetch (ext:deno_fetch/26_fetch.js:277:12)",
      "    at async fetch (ext:deno_fetch/26_fetch.js:504:7)",
      "    at async <anonymous>:2:18"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What did I just ask you?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Let's try again with a different `sessionId`. We expect to see a wholly new loaded conversation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
