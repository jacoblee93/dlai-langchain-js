{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstores and embeddings\n",
    "\n",
    "We'll continue walking through the steps of a flow to set up retrieval augmented generation. As a refresher, here are the steps we previously outlined:\n",
    "\n",
    "1. Load documents from a source.\n",
    "2. Split the docs into chunks small enough to fit into an LLM's context window and avoid distraction.\n",
    "3. Embed the chunks and store them in a vectorstore to allow for later retrieval based on input queries.\n",
    "4. Retrieval of relevant previously-split chunks.\n",
    "5. Generating a final output with retrieved chunks as context.\n",
    "\n",
    "![](./static/images/rag_diagram.png)\n",
    "\n",
    "The previous lesson covered various ways of loading and splitting documents. Next, we'll dive into the next steps of our document prepration pipeline, storage in a vectorstore.\n",
    "\n",
    "A vectorstore is a specialized type of database with natural language search capabilities. We'll show how to embed our previously split document chunks so that we can later take advantage of those capabilities.\n",
    "\n",
    "## Vectorstore ingestion\n",
    "\n",
    "Adding documents to a vectorstore is commonly called ingestion. It generally involves using another type of ML model called a text embeddings model to convert our document contents into a representation called a vector, which the vectorstore can then search over.\n",
    "\n",
    "For this lesson, we will use OpenAI's hosted embeddings and an in-memory vectorstore. For production deployments, you'll likely want to use a cloud solution which you can access from web environments. However, you can use any combination of vectorstore and embeddings you prefer.\n",
    "\n",
    "First, let's look at an embeddings model in isolation to get a sense of what it does. We'll embed a simple string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "     \u001b[33m-0.0042987\u001b[39m,  \u001b[33m0.0006434934\u001b[39m, \u001b[33m-0.0007414519\u001b[39m,  \u001b[33m-0.007843242\u001b[39m,  \u001b[33m-0.009226957\u001b[39m,\n",
       "    \u001b[33m0.015607789\u001b[39m,  \u001b[33m-0.012984631\u001b[39m,  \u001b[33m-0.002354284\u001b[39m,  \u001b[33m-0.016866904\u001b[39m,   \u001b[33m-0.01878181\u001b[39m,\n",
       "   \u001b[33m0.0010131947\u001b[39m,   \u001b[33m0.028146483\u001b[39m, \u001b[33m-0.0073186103\u001b[39m,  \u001b[33m0.0006717743\u001b[39m,   \u001b[33m0.004226563\u001b[39m,\n",
       "     \u001b[33m0.00719401\u001b[39m,   \u001b[33m0.023660883\u001b[39m,  \u001b[33m0.0021608262\u001b[39m,   \u001b[33m0.010892662\u001b[39m,  \u001b[33m-0.010715599\u001b[39m,\n",
       "  \u001b[33m-0.0034101051\u001b[39m,     \u001b[33m0.0062923\u001b[39m, \u001b[33m-0.0046331524\u001b[39m,   \u001b[33m0.016591473\u001b[39m,  \u001b[33m-0.010669694\u001b[39m,\n",
       "   \u001b[33m-0.007633389\u001b[39m,  \u001b[33m0.0010550013\u001b[39m,  \u001b[33m-0.013535494\u001b[39m,   \u001b[33m0.009856516\u001b[39m, \u001b[33m-0.0039970367\u001b[39m,\n",
       "    \u001b[33m0.012663295\u001b[39m,  \u001b[33m-0.017089874\u001b[39m, \u001b[33m-0.0022493578\u001b[39m,  \u001b[33m-0.016316041\u001b[39m,  \u001b[33m0.0035871682\u001b[39m,\n",
       "    \u001b[33m0.008794136\u001b[39m, \u001b[33m-0.0019788446\u001b[39m,  \u001b[33m-0.011240231\u001b[39m,   \u001b[33m0.026244694\u001b[39m,  \u001b[33m-0.022244377\u001b[39m,\n",
       "    \u001b[33m0.004665942\u001b[39m,   \u001b[33m0.016066842\u001b[39m,   \u001b[33m0.015778294\u001b[39m,  \u001b[33m-0.010892662\u001b[39m,  \u001b[33m-0.004459368\u001b[39m,\n",
       "   \u001b[33m-0.017627621\u001b[39m,  \u001b[33m0.0019755657\u001b[39m,  \u001b[33m-0.011345157\u001b[39m,  \u001b[33m-0.003049421\u001b[39m,   \u001b[33m0.010505747\u001b[39m,\n",
       "    \u001b[33m0.030507324\u001b[39m, \u001b[33m-0.0024362577\u001b[39m,  \u001b[33m-0.027175914\u001b[39m, \u001b[33m-0.0070497366\u001b[39m,  \u001b[33m0.0009787658\u001b[39m,\n",
       "  \u001b[33m-0.0016665249\u001b[39m,   \u001b[33m-0.02494623\u001b[39m,   \u001b[33m0.025733178\u001b[39m,   \u001b[33m0.022900168\u001b[39m,   \u001b[33m0.009863073\u001b[39m,\n",
       "   \u001b[33m-0.009823726\u001b[39m,  \u001b[33m0.0026559473\u001b[39m,  \u001b[33m0.0065874048\u001b[39m,  \u001b[33m-0.009554852\u001b[39m, \u001b[33m-0.0019657288\u001b[39m,\n",
       "    \u001b[33m-0.01837522\u001b[39m,  \u001b[33m-0.019752378\u001b[39m,  \u001b[33m0.0022247657\u001b[39m,   \u001b[33m0.009200726\u001b[39m,   \u001b[33m0.017588273\u001b[39m,\n",
       "     \u001b[33m0.02011962\u001b[39m,   \u001b[33m0.014768378\u001b[39m,  \u001b[33m-0.010735273\u001b[39m, \u001b[33m-0.0143617885\u001b[39m,   \u001b[33m0.003974084\u001b[39m,\n",
       "    \u001b[33m0.011922252\u001b[39m,  \u001b[33m-0.016066842\u001b[39m,      \u001b[33m0.013083\u001b[39m,   \u001b[33m0.011227115\u001b[39m,  \u001b[33m-0.008217041\u001b[39m,\n",
       "   \u001b[33m0.0149388835\u001b[39m,   \u001b[33m-0.04173444\u001b[39m,  \u001b[33m-0.003170742\u001b[39m,   \u001b[33m0.023096904\u001b[39m,  \u001b[33m0.0115877995\u001b[39m,\n",
       "   \u001b[33m-0.006020147\u001b[39m,   \u001b[33m0.005361079\u001b[39m,   \u001b[33m0.024382252\u001b[39m,  \u001b[33m-0.023962546\u001b[39m,  \u001b[33m-0.015437284\u001b[39m,\n",
       "  \u001b[33m-0.0033969893\u001b[39m,   \u001b[33m0.017942399\u001b[39m,  \u001b[33m0.0018181762\u001b[39m,   \u001b[33m0.021968946\u001b[39m,  \u001b[33m-0.014165052\u001b[39m,\n",
       "    \u001b[33m0.032815702\u001b[39m,   \u001b[33m-0.00632181\u001b[39m,   \u001b[33m0.035360165\u001b[39m, \u001b[33m-0.0023460868\u001b[39m,  \u001b[33m-0.035989724\u001b[39m,\n",
       "  ... 1436 more items\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings();\n",
    "\n",
    "await embeddings.embedQuery(\"This is some sample text.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a vector in the form of an array of numbers.\n",
    "\n",
    "You can think of these generated numbers as capturing various abstract features of the embedded text, and search as determining closely related vectors.\n",
    "\n",
    "For a concrete example, let's use a JavaScript library to compare similarity between some different embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33m0.6962144676957391\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { similarity } from \"ml-distance\";\n",
    "\n",
    "const vector1 = await embeddings.embedQuery(\"What are vectors useful for in machine learning?\");\n",
    "const unrelatedVector = await embeddings.embedQuery(\"A group of parrots is called a pandemonium.\");\n",
    "\n",
    "similarity.cosine(vector1, unrelatedVector);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare two more closely related texts and see what their similarity score is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33m0.8600749683959877\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const similarVector = await embeddings.embedQuery(\"Vectors are representation of information.\");\n",
    "\n",
    "similarity.cosine(vector1, similarVector);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is higher since both texts were related to something similar.\n",
    "\n",
    "We prepare documents using the techniques covered in the previous lesson. Let's set the chunk size small for demo purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Peer dependency\n",
    "import * as parse from \"pdf-parse\";\n",
    "import { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n",
    "import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n",
    "\n",
    "const loader = new PDFLoader(\"./static/docs/MachineLearning-Lecture01.pdf\");\n",
    "\n",
    "const rawCS229Docs = await loader.load();\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 128,\n",
    "  chunkOverlap: 0,\n",
    "});\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(rawCS229Docs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's initialize our vectorstore.\n",
    "\n",
    "Note that we pass in an embeddings model on initialization. The LangChain vectorstore implementation will use it to generate vector representations for each added document's content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n",
    "\n",
    "const vectorstore = new MemoryVectorStore(embeddings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally add the documents to our vectorstore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "await vectorstore.addDocuments(splitDocs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've now got a populated, searchable vectorstore!\n",
    "\n",
    "Because LangChain vectorstores expose an interface for searching directly with a natural language query, we can immediately try it and see what results we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  \u001b[32m\"piece of research in machine learning, okay?\"\u001b[39m,\n",
       "  \u001b[32m\"are using a learning algorithm, perhaps without even being aware of it.\"\u001b[39m,\n",
       "  \u001b[32m\"some of my own excitement about machine learning to you.\"\u001b[39m,\n",
       "  \u001b[32m\"of the class, and then we'll start to talk a bit about machine learning.\"\u001b[39m\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Retrieve 4 documents\n",
    "const retrievedDocs = await vectorstore.similaritySearch(\"What is deep learning?\", 4);\n",
    "\n",
    "const pageContents = retrievedDocs.map((doc) => doc.pageContent);\n",
    "\n",
    "pageContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that we get results with content related to deep learning.\n",
    "\n",
    "## Retrievers\n",
    "\n",
    "Vectorstore search is just one type of way to fetch data for an LLM. LangChain encapsulates this with a broader `Retriever` abstraction that returns documents related to a given natural language query. \n",
    "\n",
    "We can instantiate a retriever from our vectorstore with a simple function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice trait of retrievers is that unlike vectorstores, they implement `.invoke()` and are themselves Expression Language runnables, and can be chained with other modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  Document {\n",
       "    pageContent: \u001b[32m\"piece of research in machine learning, okay?\"\u001b[39m,\n",
       "    metadata: {\n",
       "      source: \u001b[32m\"./static/docs/MachineLearning-Lecture01.pdf\"\u001b[39m,\n",
       "      pdf: {\n",
       "        version: \u001b[32m\"1.10.100\"\u001b[39m,\n",
       "        info: {\n",
       "          PDFFormatVersion: \u001b[32m\"1.4\"\u001b[39m,\n",
       "          IsAcroFormPresent: \u001b[33mfalse\u001b[39m,\n",
       "          IsXFAPresent: \u001b[33mfalse\u001b[39m,\n",
       "          Title: \u001b[32m\"\"\u001b[39m,\n",
       "          Author: \u001b[32m\"\"\u001b[39m,\n",
       "          Creator: \u001b[32m\"PScript5.dll Version 5.2.2\"\u001b[39m,\n",
       "          Producer: \u001b[32m\"Acrobat Distiller 8.1.0 (Windows)\"\u001b[39m,\n",
       "          CreationDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m,\n",
       "          ModDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m\n",
       "        },\n",
       "        metadata: Metadata { _metadata: \u001b[36m[Object: null prototype]\u001b[39m },\n",
       "        totalPages: \u001b[33m22\u001b[39m\n",
       "      },\n",
       "      loc: { pageNumber: \u001b[33m8\u001b[39m, lines: { from: \u001b[33m2\u001b[39m, to: \u001b[33m2\u001b[39m } }\n",
       "    }\n",
       "  },\n",
       "  Document {\n",
       "    pageContent: \u001b[32m\"are using a learning algorithm, perhaps without even being aware of it.\"\u001b[39m,\n",
       "    metadata: {\n",
       "      source: \u001b[32m\"./static/docs/MachineLearning-Lecture01.pdf\"\u001b[39m,\n",
       "      pdf: {\n",
       "        version: \u001b[32m\"1.10.100\"\u001b[39m,\n",
       "        info: {\n",
       "          PDFFormatVersion: \u001b[32m\"1.4\"\u001b[39m,\n",
       "          IsAcroFormPresent: \u001b[33mfalse\u001b[39m,\n",
       "          IsXFAPresent: \u001b[33mfalse\u001b[39m,\n",
       "          Title: \u001b[32m\"\"\u001b[39m,\n",
       "          Author: \u001b[32m\"\"\u001b[39m,\n",
       "          Creator: \u001b[32m\"PScript5.dll Version 5.2.2\"\u001b[39m,\n",
       "          Producer: \u001b[32m\"Acrobat Distiller 8.1.0 (Windows)\"\u001b[39m,\n",
       "          CreationDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m,\n",
       "          ModDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m\n",
       "        },\n",
       "        metadata: Metadata { _metadata: \u001b[36m[Object: null prototype]\u001b[39m },\n",
       "        totalPages: \u001b[33m22\u001b[39m\n",
       "      },\n",
       "      loc: { pageNumber: \u001b[33m3\u001b[39m, lines: { from: \u001b[33m39\u001b[39m, to: \u001b[33m39\u001b[39m } }\n",
       "    }\n",
       "  },\n",
       "  Document {\n",
       "    pageContent: \u001b[32m\"some of my own excitement about machine learning to you.\"\u001b[39m,\n",
       "    metadata: {\n",
       "      source: \u001b[32m\"./static/docs/MachineLearning-Lecture01.pdf\"\u001b[39m,\n",
       "      pdf: {\n",
       "        version: \u001b[32m\"1.10.100\"\u001b[39m,\n",
       "        info: {\n",
       "          PDFFormatVersion: \u001b[32m\"1.4\"\u001b[39m,\n",
       "          IsAcroFormPresent: \u001b[33mfalse\u001b[39m,\n",
       "          IsXFAPresent: \u001b[33mfalse\u001b[39m,\n",
       "          Title: \u001b[32m\"\"\u001b[39m,\n",
       "          Author: \u001b[32m\"\"\u001b[39m,\n",
       "          Creator: \u001b[32m\"PScript5.dll Version 5.2.2\"\u001b[39m,\n",
       "          Producer: \u001b[32m\"Acrobat Distiller 8.1.0 (Windows)\"\u001b[39m,\n",
       "          CreationDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m,\n",
       "          ModDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m\n",
       "        },\n",
       "        metadata: Metadata { _metadata: \u001b[36m[Object: null prototype]\u001b[39m },\n",
       "        totalPages: \u001b[33m22\u001b[39m\n",
       "      },\n",
       "      loc: { pageNumber: \u001b[33m4\u001b[39m, lines: { from: \u001b[33m24\u001b[39m, to: \u001b[33m24\u001b[39m } }\n",
       "    }\n",
       "  },\n",
       "  Document {\n",
       "    pageContent: \u001b[32m\"of the class, and then we'll start to talk a bit about machine learning.\"\u001b[39m,\n",
       "    metadata: {\n",
       "      source: \u001b[32m\"./static/docs/MachineLearning-Lecture01.pdf\"\u001b[39m,\n",
       "      pdf: {\n",
       "        version: \u001b[32m\"1.10.100\"\u001b[39m,\n",
       "        info: {\n",
       "          PDFFormatVersion: \u001b[32m\"1.4\"\u001b[39m,\n",
       "          IsAcroFormPresent: \u001b[33mfalse\u001b[39m,\n",
       "          IsXFAPresent: \u001b[33mfalse\u001b[39m,\n",
       "          Title: \u001b[32m\"\"\u001b[39m,\n",
       "          Author: \u001b[32m\"\"\u001b[39m,\n",
       "          Creator: \u001b[32m\"PScript5.dll Version 5.2.2\"\u001b[39m,\n",
       "          Producer: \u001b[32m\"Acrobat Distiller 8.1.0 (Windows)\"\u001b[39m,\n",
       "          CreationDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m,\n",
       "          ModDate: \u001b[32m\"D:20080711112523-07'00'\"\u001b[39m\n",
       "        },\n",
       "        metadata: Metadata { _metadata: \u001b[36m[Object: null prototype]\u001b[39m },\n",
       "        totalPages: \u001b[33m22\u001b[39m\n",
       "      },\n",
       "      loc: { pageNumber: \u001b[33m1\u001b[39m, lines: { from: \u001b[33m4\u001b[39m, to: \u001b[33m4\u001b[39m } }\n",
       "    }\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await retriever.invoke(\"What is deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take advantage of this in the next lesson in combination with what we learned in the first lessons to create a retrieval chain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
