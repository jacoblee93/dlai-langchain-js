{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web endpoints\n",
    "\n",
    "Now that we've designed a simple retrieval chain, let's look at what it would take to productionize it as a streaming chat endpoint!\n",
    "\n",
    "We'll go over the interaction with native web primitives like `fetch` and `Response`, as well as show how to utilize different chat sessions.\n",
    "\n",
    "We'll pick up where we left off in the last lesson with loading and splitting our CS229 transcript into a vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  loadAndSplitChunks, \n",
    "  initializeVectorstoreWithDocuments \n",
    "} from \"./lib/helpers.ts\";\n",
    "\n",
    "const splitDocs = await loadAndSplitChunks({\n",
    "  chunkSize: 1536,\n",
    "  chunkOverlap: 128,\n",
    "});\n",
    "\n",
    "const vectorstore = await initializeVectorstoreWithDocuments({\n",
    "  documents: splitDocs,\n",
    "});\n",
    "\n",
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pieces of our conversational retrieval chain together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  createDocumentRetrievalChain, \n",
    "  createRephraseQuestionChain \n",
    "} from \"./lib/helpers.ts\";\n",
    "\n",
    "const documentRetrievalChain = createDocumentRetrievalChain();\n",
    "const rephraseQuestionChain = createRephraseQuestionChain();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\n",
    "\n",
    "const ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are an experienced researcher,\n",
    "expert at interpreting and answering questions based on provided sources.\n",
    "Using the below provided context and chat history, \n",
    "answer the user's question to the best of your ability\n",
    "using only the resources provided. Be verbose!\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>`;\n",
    "\n",
    "const HUMAN_MESSAGE_TEMPLATE = \n",
    "  `Now, answer this question using the previous context and chat history:\n",
    "  \n",
    "  {standalone_question}`;\n",
    "\n",
    "const answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", ANSWER_CHAIN_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\n",
    "    \"human\", \n",
    "    HUMAN_MESSAGE_TEMPLATE\n",
    "  ]\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we assemble all the pieces together, let's note that the native web `Response` objects used to return data in popular frameworks like Next.js accept a `ReadableStream` the emits bytes directly. Previously, our chain outputted string chunks directly using `StringOutputParser`, but it would be convenient to be able to directly stream so that we could pass our LangChain stream directly to the response.\n",
    "\n",
    "Fortunately, LangChain provides an `HttpResponseOutputParser` that parses chat output into chunks of bytes that match either `text/plain` or `text/event-stream` content types! To use it, let's construct our conversational retrieval chain as before, but skip the final `StringOutputParser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  RunnablePassthrough, \n",
    "  RunnableSequence \n",
    "} from \"langchain/runnables\";\n",
    "import { ChatOpenAI } from \"langchain/chat_models/openai\";\n",
    "\n",
    "const conversationalRetrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    standalone_question: rephraseQuestionChain,\n",
    "  }),\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationChainPrompt,\n",
    "  new ChatOpenAI({ modelName: \"gpt-3.5-turbo-1106\" }),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create an `HttpResponseOutputParser` and pipe the `RunnableWithMessageHistory` into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HttpResponseOutputParser } from \"langchain/output_parsers\";\n",
    "import { RunnableWithMessageHistory } from \"langchain/runnables\"; \n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "\n",
    "// \"text/event-stream\" is also supported\n",
    "const httpResponseOutputParser = new HttpResponseOutputParser({\n",
    "  contentType: \"text/plain\"\n",
    "});\n",
    "\n",
    "const messageHistoryMap = new Map();\n",
    "\n",
    "const finalRetrievalChain = new RunnableWithMessageHistory({\n",
    "  runnable: conversationalRetrievalChain,\n",
    "  // Mention where sessionId gets passed from (parameter to our endpoint)\n",
    "  getMessageHistory: (sessionId) => {\n",
    "    if (sessionId in messageHistoryMap) {\n",
    "      return messageHistoryMap.get(sessionId);\n",
    "    }\n",
    "    const newChatSessionHistory = new ChatMessageHistory();\n",
    "    messageHistoryMap.set(sessionId, newChatSessionHistory);\n",
    "    return newChatSessionHistory;\n",
    "  },\n",
    "  inputMessagesKey: \"question\",\n",
    "  historyMessagesKey: \"history\",\n",
    "}).pipe(httpResponseOutputParser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we don't put the `HttpResponseOutputParser` directly in the `conversationalRetrievalChain` is because `RunnableWithMessageHistory` will store the aggregated output of its runnable in the `ChatMessageHistory`, and requires either a string or a `ChatMessage` to be the final output rather than bytes.\n",
    "\n",
    "You might also notice that our `getMessageHistory` function creates a new `ChatMessageHistory` object based on the passed `sessionId` instead of reusing the same one as before. This allows us to assign `sessionId`s properly to individual conversations and load them as requests come in later. For more advanced persistence, you'll want to use a integration to store these histories.\n",
    "\n",
    "Great! Let's set up a simple server with a handler that calls our chain and see if we can get a streaming response. We'll populate the input question and the session id from the body parameters. Since this notebook is written in Deno, we use a Deno built-in HTTP method, but this general concept is shared by many JS frameworks.\n",
    "\n",
    "Also, in a true production deployment, you'd likely want to set up authentication/input validation via some middleware, but we'll skip that for simplicity for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP server is running! Access it at: http://localhost:8080/\n",
      "Listening on http://localhost:8080/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  finished: Promise { \u001b[36m<pending>\u001b[39m },\n",
       "  shutdown: \u001b[36m[AsyncFunction: shutdown]\u001b[39m,\n",
       "  ref: \u001b[36m[Function: ref]\u001b[39m,\n",
       "  unref: \u001b[36m[Function: unref]\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const port = 8080;\n",
    "\n",
    "const handler = async (request: Request): Response => {\n",
    "  const body = await request.json();\n",
    "  const stream = await finalRetrievalChain.stream({\n",
    "    question: body.question\n",
    "  }, { configurable: { sessionId: body.session_id } });\n",
    "\n",
    "  return new Response(stream, { \n",
    "    status: 200,\n",
    "    headers: {\n",
    "      \"Content-Type\": \"text/plain\"\n",
    "    },\n",
    "  });\n",
    "};\n",
    "\n",
    "console.log(`HTTP server is running! Access it at: http://localhost:${port}/`);\n",
    "Deno.serve({ port }, handler);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a quick helper function to make handling the response stream in the client a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const decoder = new TextDecoder();\n",
    "\n",
    "// readChunks() reads from the provided reader and yields the results into an async iterable\n",
    "function readChunks(reader) {\n",
    "  return {\n",
    "    async* [Symbol.asyncIterator]() {\n",
    "      let readResult = await reader.read();\n",
    "      while (!readResult.done) {\n",
    "        yield decoder.decode(readResult.value);\n",
    "        readResult = await reader.read();\n",
    "      }\n",
    "    },\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try calling our endpoint!\n",
    "\n",
    "We use a sleep function at the end due to the limitations of running a server within a notebook - we want to make sure the request finishes before the cell stop execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: The course has speci\n",
      "CHUNK: fic requirements \n",
      "CHUNK: for the students. The instruct\n",
      "CHUNK: or, Andrew Ng, \n",
      "CHUNK: mentioned that the course w\n",
      "CHUNK: ill not be very p\n",
      "CHUNK: rogramming in\n",
      "CHUNK: tensive, but it will invol\n",
      "CHUNK: ve some\n",
      "CHUNK:  programming, mostly us\n",
      "CHUNK: ing MATLAB or Octave.\n",
      "CHUNK:  He also assumes fami\n",
      "CHUNK: liarity wit\n",
      "CHUNK: h basic probability \n",
      "CHUNK: and statistics, su\n",
      "CHUNK: ggesting \n",
      "CHUNK: that most undergr\n",
      "CHUNK: aduate st\n",
      "CHUNK: atistics classes like \n",
      "CHUNK: Stat 116 at\n",
      "CHUNK:  Stanford will be more\n",
      "CHUNK:  than enough to mee\n",
      "CHUNK: t this requirement. Additionally,\n",
      "CHUNK:  students are expect\n",
      "CHUNK: ed to have an underst\n",
      "CHUNK: anding of\n",
      "CHUNK:  basic linear algebra, \n",
      "CHUNK: which can be ac\n",
      "CHUNK: quired through \n",
      "CHUNK: courses like Math 51, \n",
      "CHUNK: 103, Math 11\n",
      "CHUNK: 3, or CS205 at Stanford\n",
      "CHUNK: .\n",
      "\n",
      "The instructor also makes\n",
      "CHUNK:  it clear that students sh\n",
      "CHUNK: ould be \n",
      "CHUNK: familiar with concepts such as random variables,\n",
      "CHUNK:  expectation, variance, matrices, ve\n",
      "CHUNK: ctors, matrix multiplic\n",
      "CHUNK: ation, matrix invers\n",
      "CHUNK: es, and eigenvectors\n",
      "CHUNK: . He mentions\n",
      "CHUNK:  that a refresher\n",
      "CHUNK:  course on some of\n",
      "CHUNK:  these prerequisites\n",
      "CHUNK:  will be covered in\n",
      "CHUNK:  discuss\n",
      "CHUNK: ion sections for those\n",
      "CHUNK:  who need it.\n",
      "\n",
      "Regar\n",
      "CHUNK: ding programming languages, the instructor \n",
      "CHUNK: emphasizes that the\n",
      "CHUNK: re is no C\n",
      "CHUNK:  programming in the class, ex\n",
      "CHUNK: cept for any that \n",
      "CHUNK: students may choose to do themselv\n",
      "CHUNK: es in their proje\n",
      "CHUNK: cts. All homework\n",
      "CHUNK:  can be done in MATLAB or Octave,\n",
      "CHUNK:  and the prerequisites for \n",
      "CHUNK: programming are understanding \n",
      "CHUNK: big-O notation and k\n",
      "CHUNK: nowledge of data \n",
      "CHUNK: structures such as linked lists, queues, and binary treatments.\n",
      "\n",
      "In summary, the requirements for the course include a basic understanding of probability and statistics, familiarity with linear algebra, and some knowledge of programming concepts and languages such as MATLAB or Octave.\n"
     ]
    }
   ],
   "source": [
    "const sleep = async () => {\n",
    "  return new Promise((resolve) => setTimeout(resolve, 500));\n",
    "}\n",
    "\n",
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What are the prerequisites for this course?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get a streamed string response.\n",
    "\n",
    "Now, let's test the memory by asking a followup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: Based on the provided con\n",
      "CHUNK: text, the reques\n",
      "CHUNK: t for listing \n",
      "CHUNK: in bullet poin\n",
      "CHUNK: t format is \n",
      "CHUNK: unclear. Howe\n",
      "CHUNK: ver, if we consider\n",
      "CHUNK:  the prior conversatio\n",
      "CHUNK: ns and the conten\n",
      "CHUNK: t of the documen\n",
      "CHUNK: ts, it seems likely t\n",
      "CHUNK: hat the user is a\n",
      "CHUNK: sking for a list\n",
      "CHUNK:  related to a spe\n",
      "CHUNK: cific top\n",
      "CHUNK: ic or info\n",
      "CHUNK: rmation. Without more\n",
      "CHUNK:  clarity\n",
      "CHUNK:  on the specific list the user is \n",
      "CHUNK: requesting, I'm\n",
      "CHUNK:  unable to provide\n",
      "CHUNK:  a precise bullet-point list without m\n",
      "CHUNK: aking assu\n",
      "CHUNK: mptions.\n",
      "\n",
      "If the user \n",
      "CHUNK: could provide\n",
      "CHUNK:  more details about the su\n",
      "CHUNK: bject or conte\n",
      "CHUNK: xt they are refer\n",
      "CHUNK: ring to, I would be h\n",
      "CHUNK: appy to assist in creating\n",
      "CHUNK:  a bullet-point list. This \n",
      "CHUNK: could include a list of\n",
      "CHUNK:  course topics, examp\n",
      "CHUNK: les of machine learning projects, a list of students' academic backgrounds in a classroom, or a list of reminders given by the instructor at the end of a class. With more information provided, I'll be able to create a more relevant and accurate bullet-point list.\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"Can you list them in bullet point format?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Let's try again with a different `sessionId`. We expect to see a wholly new loaded conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: Based on the context an\n",
      "CHUNK: d chat history pr\n",
      "CHUNK: ovided, the previ\n",
      "CHUNK: ous question you asked r\n",
      "CHUNK: evolved around the diversity of the students in the class. Specifically, you were inquiring about the backgrounds of the students in the class and where they were from. This demonstrates a curiosity about the diversity of the audience and the various disciplines they represent.\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What did I just ask you?\",\n",
    "    session_id: \"2\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entirely new session! While the current version of the Deno kernel can't currently render a frontend in the notebook, you could update a frontend component with the content of the stream to create a responsive chat experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
