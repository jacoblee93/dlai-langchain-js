{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web endpoints\n",
    "\n",
    "Now that we've designed a simple retrieval chain, let's look at what it would take to productionize it as a streaming chat endpoint!\n",
    "\n",
    "We'll go over the interaction with native web primitives like `fetch` and `Response`, as well as show how to utilize different chat sessions.\n",
    "\n",
    "We'll pick up where we left off in the last lesson with loading and splitting our CS229 transcript into a vectorstore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  loadAndSplitChunks, \n",
    "  initializeVectorstoreWithDocuments \n",
    "} from \"./lib/helpers.ts\";\n",
    "\n",
    "const splitDocs = await loadAndSplitChunks({\n",
    "  chunkSize: 1536,\n",
    "  chunkOverlap: 128,\n",
    "});\n",
    "\n",
    "const vectorstore = await initializeVectorstoreWithDocuments({\n",
    "  documents: splitDocs,\n",
    "});\n",
    "\n",
    "const retriever = vectorstore.asRetriever();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pieces of our conversational retrieval chain together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  createDocumentRetrievalChain, \n",
    "  createRephraseQuestionChain \n",
    "} from \"./lib/helpers.ts\";\n",
    "\n",
    "const documentRetrievalChain = createDocumentRetrievalChain();\n",
    "const rephraseQuestionChain = createRephraseQuestionChain();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"langchain/prompts\";\n",
    "\n",
    "const ANSWER_CHAIN_SYSTEM_TEMPLATE = `You are an experienced researcher,\n",
    "expert at interpreting and answering questions based on provided sources.\n",
    "Using the below provided context and chat history, \n",
    "answer the user's question to the best of your ability\n",
    "using only the resources provided. Be verbose!\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>`;\n",
    "\n",
    "const HUMAN_MESSAGE_TEMPLATE = \n",
    "  `Now, answer this question using the previous context and chat history:\n",
    "  \n",
    "  {standalone_question}`;\n",
    "\n",
    "const answerGenerationChainPrompt = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", ANSWER_CHAIN_SYSTEM_TEMPLATE],\n",
    "  new MessagesPlaceholder(\"history\"),\n",
    "  [\n",
    "    \"human\", \n",
    "    HUMAN_MESSAGE_TEMPLATE\n",
    "  ]\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we assemble all the pieces together, let's note that the native web `Response` objects used to return data in popular frameworks like Next.js accept a `ReadableStream` the emits bytes directly. Previously, our chain outputted string chunks directly using `StringOutputParser`, but it would be convenient to be able to directly stream so that we could pass our LangChain stream directly to the response.\n",
    "\n",
    "Fortunately, LangChain provides an `HttpResponseOutputParser` that parses chat output into chunks of bytes that match either `text/plain` or `text/event-stream` content types! To use it, let's construct our conversational retrieval chain as before, but skip the final `StringOutputParser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { \n",
    "  RunnablePassthrough, \n",
    "  RunnableSequence \n",
    "} from \"langchain/runnables\";\n",
    "import { ChatOpenAI } from \"langchain/chat_models/openai\";\n",
    "\n",
    "const conversationalRetrievalChain = RunnableSequence.from([\n",
    "  RunnablePassthrough.assign({\n",
    "    standalone_question: rephraseQuestionChain,\n",
    "  }),\n",
    "  RunnablePassthrough.assign({\n",
    "    context: documentRetrievalChain,\n",
    "  }),\n",
    "  answerGenerationChainPrompt,\n",
    "  new ChatOpenAI({ modelName: \"gpt-3.5-turbo-1106\" }),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create an `HttpResponseOutputParser` and pipe the `RunnableWithMessageHistory` into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { HttpResponseOutputParser } from \"langchain/output_parsers\";\n",
    "import { RunnableWithMessageHistory } from \"langchain/runnables\"; \n",
    "import { ChatMessageHistory } from \"langchain/stores/message/in_memory\";\n",
    "\n",
    "// \"text/event-stream\" is also supported\n",
    "const httpResponseOutputParser = new HttpResponseOutputParser({\n",
    "  contentType: \"text/plain\"\n",
    "});\n",
    "\n",
    "const messageHistories = {};\n",
    "\n",
    "const finalRetrievalChain = new RunnableWithMessageHistory({\n",
    "  runnable: conversationalRetrievalChain,\n",
    "  // Mention where sessionId gets passed from (parameter to our endpoint)\n",
    "  getMessageHistory: (sessionId) => {\n",
    "    if (sessionId in messageHistories) {\n",
    "      return messageHistories[sessionId];\n",
    "    }\n",
    "    const newChatSessionHistory = new ChatMessageHistory();\n",
    "    messageHistories[sessionId] = newChatSessionHistory;\n",
    "    return newChatSessionHistory;\n",
    "  },\n",
    "  inputMessagesKey: \"question\",\n",
    "  historyMessagesKey: \"history\",\n",
    "}).pipe(httpResponseOutputParser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we don't put the `HttpResponseOutputParser` directly in the `conversationalRetrievalChain` is because `RunnableWithMessageHistory` will store the aggregated output of its runnable in the `ChatMessageHistory`, and requires either a string or a `ChatMessage` to be the final output rather than bytes.\n",
    "\n",
    "You might also notice that our `getMessageHistory` function creates a new `ChatMessageHistory` object based on the passed `sessionId` instead of reusing the same one as before. This allows us to assign `sessionId`s properly to individual conversations and load them as requests come in later. For more advanced persistence, you'll want to use a integration to store these histories.\n",
    "\n",
    "Great! Let's set up a simple server with a handler that calls our chain and see if we can get a streaming response. We'll populate the input question and the session id from the body parameters. Since this notebook is written in Deno, we use a Deno built-in HTTP method, but this general concept is shared by many JS frameworks.\n",
    "\n",
    "Also, in a true production deployment, you'd likely want to set up authentication/input validation via some middleware, but we'll skip that for simplicity for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP server is running! Access it at: http://localhost:8080/\n",
      "Listening on http://localhost:8080/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  finished: Promise { \u001b[36m<pending>\u001b[39m },\n",
       "  shutdown: \u001b[36m[AsyncFunction: shutdown]\u001b[39m,\n",
       "  ref: \u001b[36m[Function: ref]\u001b[39m,\n",
       "  unref: \u001b[36m[Function: unref]\u001b[39m\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const port = 8080;\n",
    "\n",
    "const handler = async (request: Request): Response => {\n",
    "  const body = await request.json();\n",
    "  const stream = await finalRetrievalChain.stream({\n",
    "    question: body.question\n",
    "  }, { configurable: { sessionId: body.session_id } });\n",
    "\n",
    "  return new Response(stream, { \n",
    "    status: 200,\n",
    "    headers: {\n",
    "      \"Content-Type\": \"text/plain\"\n",
    "    },\n",
    "  });\n",
    "};\n",
    "\n",
    "console.log(`HTTP server is running! Access it at: http://localhost:${port}/`);\n",
    "Deno.serve({ port }, handler);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a quick helper function to make handling the response stream in the client a bit nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const decoder = new TextDecoder();\n",
    "\n",
    "// readChunks() reads from the provided reader and yields the results into an async iterable\n",
    "function readChunks(reader) {\n",
    "  return {\n",
    "    async* [Symbol.asyncIterator]() {\n",
    "      let readResult = await reader.read();\n",
    "      while (!readResult.done) {\n",
    "        yield decoder.decode(readResult.value);\n",
    "        readResult = await reader.read();\n",
    "      }\n",
    "    },\n",
    "  };\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try calling our endpoint!\n",
    "\n",
    "We use a sleep function at the end due to the limitations of running a server within a notebook - we want to make sure the request finishes before the cell stop execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "request or response body error: error reading a body from connection: unexpected EOF during chunk size line",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: request or response body error: error reading a body from connection: unexpected EOF during chunk size line",
      "    at async Object.pull (ext:deno_web/06_streams.js:905:27)"
     ]
    }
   ],
   "source": [
    "const sleep = async () => {\n",
    "  return new Promise((resolve) => setTimeout(resolve, 500));\n",
    "}\n",
    "\n",
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What are the prerequisites for this course?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get a streamed string response.\n",
    "\n",
    "Now, let's test the memory by asking a followup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: Based on the chat hist\n",
      "CHUNK: ory provided, it seems like\n",
      "CHUNK:  the user is asking \n",
      "CHUNK: for a specific list. Fr\n",
      "CHUNK: om the conve\n",
      "CHUNK: rsation and documen\n",
      "CHUNK: ts provided, it is not cle\n",
      "CHUNK: ar what exact list\n",
      "CHUNK:  the user is referring to. Howe\n",
      "CHUNK: ver, I can make\n",
      "CHUNK:  an assump\n",
      "CHUNK: tion based on the context.\n",
      "\n",
      "Assuming t\n",
      "CHUNK: he user is referring to a list of topics discussed i\n",
      "CHUNK: n the conversation and documents provided, the bullet point\n",
      "CHUNK:  format could look\n",
      "CHUNK:  like this:\n",
      "\n",
      "- Online Resources:\n",
      "CHUNK:  This section seems t\n",
      "CHUNK: o be mentioned on the thi\n",
      "CHUNK: rd page of a\n",
      "CHUNK:  course handou\n",
      "CHUNK: t and may contain valu\n",
      "CHUNK: able info\n",
      "CHUNK: rmation related\n",
      "CHUNK:  to the co\n",
      "CHUNK: urse.\n",
      "- Learning Algo\n",
      "CHUNK: rithms: This was a topic mentioned \n",
      "CHUNK: in the chat history where the instructor talks about teaching machine lear\n",
      "CHUNK: ning algorithms to enable a car\n",
      "CHUNK:  to drive off ro\n",
      "CHUNK: ads at high speeds or a \n",
      "CHUNK: robot to overcome ob\n",
      "CHUNK: stacles.\n",
      "- Student B\n",
      "CHUNK: ackground\n",
      "CHUNK: s: This is not a specific section, but based on the chat history, it seems there was a discussion about the diverse academic backgrounds of the s\n",
      "CHUNK: tudents in the class, incl\n",
      "CHUNK: uding area\n",
      "CHUNK: s like iCME, statistics, \n",
      "CHUNK: civil engineering\n",
      "CHUNK: , systems sy\n",
      "CHUNK: nthesis, chemistry, aero/astro, industry, and more.\n",
      "\n",
      "These are the key points mentioned in the conversation and the documents provided. If there is a specific list the user is referring to, it would be helpful to provide more context or clarify the topic or list they are interested in.\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"Can you list them in bullet point format?\",\n",
    "    session_id: \"1\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Let's try again with a different `sessionId`. We expect to see a wholly new loaded conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK: Based on the p\n",
      "CHUNK: rovided chat histor\n",
      "CHUNK: y and context, I can't\n",
      "CHUNK:  determine\n",
      "CHUNK:  the exact question\n",
      "CHUNK:  you asked \n",
      "CHUNK: that prompted th\n",
      "CHUNK: e response by the in\n",
      "CHUNK: structor, A\n",
      "CHUNK: ndrew Ng. The conver\n",
      "CHUNK: sation primar\n",
      "CHUNK: ily revolve\n",
      "CHUNK: s around various \n",
      "CHUNK: topics, including\n",
      "CHUNK:  forming study groups,\n",
      "CHUNK:  course resources,\n",
      "CHUNK:  and the diverse ba\n",
      "CHUNK: ckgrounds of the \n",
      "CHUNK: students in the class. It seems that your question may not be captured in the provided context.\n",
      "\n",
      "If you have a specific question in mind that you'd like assistance with, please feel free to provide additional details or directly ask the question, and I'll be happy to help!\n"
     ]
    }
   ],
   "source": [
    "const response = await fetch(\"http://localhost:8080/\", {\n",
    "  method: \"POST\",\n",
    "  headers: {\n",
    "    \"content-type\": \"application/json\",\n",
    "  },\n",
    "  body: JSON.stringify({\n",
    "    question: \"What did I just ask you?\",\n",
    "    session_id: \"2\", // Should randomly generate/assign\n",
    "  })\n",
    "});\n",
    "\n",
    "// response.body is a ReadableStream\n",
    "const reader = response.body?.getReader();\n",
    "\n",
    "for await (const chunk of readChunks(reader)) {\n",
    "  console.log(\"CHUNK:\", chunk);\n",
    "}\n",
    "\n",
    "await sleep();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entirely new session! While the current version of the Deno kernel can't currently render a frontend in the notebook, you could update a frontend component with the content of the stream to create a responsive chat experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
